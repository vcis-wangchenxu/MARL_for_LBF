import torch
import torch.nn as nn
import torch.nn.functional as F

class QMixer(nn.Module):
    """
    Mixing Network for QMIX.
    Takes agent Q-values and global state as input, outputs Q_tot.
    Weights are generated by Hypernetworks dependent on state.
    Enforces monotonicity: weights >= 0.
    """
    def __init__(self, n_agents, state_shape, mixing_embed_dim=32, hypernet_embed=64):
        super(QMixer, self).__init__()
        self.n_agents = n_agents
        self.state_shape = state_shape # (C, H, W)
        self.embed_dim = mixing_embed_dim

        C, H, W = state_shape    # C <- N*C
        self.conv = nn.Sequential(
            nn.Conv2d(C, 16, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Flatten()
        )
        # Calculate output dim
        self.state_dim = 32 * H * W 
        
        # Hypernetworks
        # Hyper w1: (state) -> (N * embed)
        self.hyper_w1 = nn.Sequential(
            nn.Linear(self.state_dim, hypernet_embed),
            nn.ReLU(),
            nn.Linear(hypernet_embed, n_agents * self.embed_dim)
        )
        # Hyper b1: (state) -> (embed)
        self.hyper_b1 = nn.Linear(self.state_dim, self.embed_dim)
        
        # Hyper w2: (state) -> (embed * 1)
        self.hyper_w2 = nn.Sequential(
            nn.Linear(self.state_dim, hypernet_embed),
            nn.ReLU(),
            nn.Linear(hypernet_embed, self.embed_dim)
        )
        # Hyper b2: (state) -> (1)
        self.hyper_b2 = nn.Sequential(
            nn.Linear(self.state_dim, hypernet_embed),
            nn.ReLU(),
            nn.Linear(hypernet_embed, 1)
        )

    def forward(self, agent_qs, states):
        """
        Args:
            agent_qs (torch.Tensor): (Batch, Seq_Len, N_Agents)
            states (torch.Tensor): (Batch, Seq_Len, *State_Shape)
        Returns:
            q_tot (torch.Tensor): (Batch, Seq_Len, 1)
        """
        B, L, N = agent_qs.shape
        
        agent_qs = agent_qs.view(-1, 1, N) # (B*L, 1, N)
        
        # states: (B, L, C, H, W) -> (B*L, C, H, W)
        states = states.reshape(-1, *self.state_shape)
        states = self.conv(states) # (B*L, state_dim)

        # First layer
        w1 = torch.abs(self.hyper_w1(states)) # (B*L, N * embed)
        b1 = self.hyper_b1(states) # (B*L, embed)
        
        w1 = w1.view(-1, N, self.embed_dim) # (B*L, N, embed)
        b1 = b1.view(-1, 1, self.embed_dim) # (B*L, 1, embed)
        
        # (B*T, 1, N) * (B*T, N, embed) -> (B*T, 1, embed)
        hidden = F.elu(torch.bmm(agent_qs, w1) + b1)
        
        # Second layer
        w2 = torch.abs(self.hyper_w2(states)) # (B*L, embed)
        b2 = self.hyper_b2(states) # (B*L, 1)
        
        w2 = w2.view(-1, self.embed_dim, 1) # (B*L, embed, 1)
        b2 = b2.view(-1, 1, 1)
        
        # (B*T, 1, embed) * (B*T, embed, 1) -> (B*T, 1, 1)
        y = torch.bmm(hidden, w2) + b2
        
        q_tot = y.view(B, T, 1)
        return q_tot
